{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJTZSWdDMzV2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "def create_mlp(activation):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(256, activation=activation, input_shape=(784,)),\n",
        "        layers.Dense(128, activation=activation),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "activations = ['sigmoid', 'tanh', 'relu']\n",
        "histories = {}\n",
        "train_times = {}\n",
        "\n",
        "for act in activations:\n",
        "    print(f\"Training model with {act} activation...\")\n",
        "    model = create_mlp(act)\n",
        "    start = time.time()\n",
        "    history = model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
        "                        epochs=10, batch_size=128, verbose=0)\n",
        "    end = time.time()\n",
        "    histories[act] = history\n",
        "    train_times[act] = end - start\n",
        "\n",
        "\n",
        "\n",
        "def compute_gradients(model, x_sample, y_sample):\n",
        "    with tf.GradientTape() as tape:\n",
        "        preds = model(x_sample, training=True)\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_sample, preds)\n",
        "    grads = tape.gradient(loss, model.trainable_weights)\n",
        "    grad_norms = [tf.norm(g) for g in grads if g is not None]\n",
        "    return tf.reduce_mean(grad_norms).numpy()\n",
        "\n",
        "x_sample = x_train[:256]\n",
        "y_sample = y_train[:256]\n",
        "grad_means = {}\n",
        "\n",
        "for act in activations:\n",
        "    model = create_mlp(act)\n",
        "    model.fit(x_sample, y_sample, epochs=1, batch_size=64, verbose=0)\n",
        "    grad_means[act] = compute_gradients(model, x_sample, y_sample)\n",
        "\n",
        "\n",
        "def plot_metric(metric, title):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for act in activations:\n",
        "        plt.plot(histories[act].history[metric], label=f\"{act} Train\")\n",
        "        plt.plot(histories[act].history[f'val_{metric}'], '--', label=f\"{act} Val\")\n",
        "    plt.title(f\"{title}\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_metric('accuracy', \"Training and Validation Accuracy Comparison\")\n",
        "plot_metric('loss', \"Training and Validation Loss Comparison\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(activations, [train_times[a] for a in activations], color=['blue','green','red'])\n",
        "plt.title(\"Training Speed Comparison (Lower is Faster)\")\n",
        "plt.ylabel(\"Time (seconds)\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(grad_means.keys(), grad_means.values(), color=['blue','green','red'])\n",
        "plt.title(\"Average Gradient Norm per Activation\")\n",
        "plt.ylabel(\"Gradient Magnitude\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "for act in activations:\n",
        "    model = create_mlp(act)\n",
        "    model.fit(x_train, y_train, epochs=5, batch_size=128, verbose=0)\n",
        "    test_acc = model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "    print(f\"{act.capitalize()} Test Accuracy: {test_acc:.4f}\")"
      ]
    }
  ]
}